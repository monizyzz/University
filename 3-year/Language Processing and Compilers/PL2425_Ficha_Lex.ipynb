{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GdHi2G0OlbE"
      },
      "source": [
        "# Ficha de Análise Léxica (Lex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0JdE83EOlb0"
      },
      "source": [
        "## Introdução"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC00B5-IOlb1"
      },
      "source": [
        "A análise léxica é o processo de conversão de uma sequência de caracteres numa sequência de *tokens*, em que cada *token* representa uma unidade significativa da linguagem à qual os caracteres pertencem.\n",
        "\n",
        "Em Python, podemos fazer análise léxica de várias formas. A que iremos utilizar nas aulas recorre ao módulo Ply, que para além de análise léxica vai-nos permitir fazer análise sintática.\n",
        "\n",
        "Antes de usar o módulo Ply, precisamos de o instalar. Para isso, podemos usar o comando seguinte:\n",
        "\n",
        "```sh\n",
        "$ pip install ply\n",
        "```\n",
        "\n",
        "Depois, apenas precisamos de importar a ferramenta `lex.py` no nosso programa:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yXKYfAnOlcJ"
      },
      "outputs": [],
      "source": [
        "import ply.lex as lex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx_dqWwkOlcL"
      },
      "source": [
        "A primeira coisa que o nosso analisador léxico (ou *lexer*/*tokenizer*) precisa de ter é uma lista de *tokens*. Como exemplo, vamos definir um *lexer* que lê expressões aritméticas, como \"4 * (2 + 3)\". Neste exemplo já somos capazes de identificar alguns *tokens*..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgXrbPicOlcM"
      },
      "outputs": [],
      "source": [
        "tokens = (\n",
        "    'NUMBER',\n",
        "    'PLUS',\n",
        "    # completar...\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsNXH-ZeOlcM"
      },
      "source": [
        "A seguir é preciso especificar cada *token*. Por outras palavras, precisamos de definir expressões regulares que permitam ao *tokenizer* identificar os *tokens*. Podemos fazê-lo através de variáveis ou de funções."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F95mz8RYOlcb"
      },
      "outputs": [],
      "source": [
        "t_PLUS = r'\\+'\n",
        "# completar...\n",
        "\n",
        "def t_NUMBER(t):\n",
        "  r'\\d+'\n",
        "  t.value = int(t.value)\n",
        "  return t\n",
        "\n",
        "    # completar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow2ILFtkOlcc"
      },
      "source": [
        "Podemos especificar um conjunto de caracteres que o analisador léxico vai ignorar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fxalLuTOlcd"
      },
      "outputs": [],
      "source": [
        "t_ignore = ' \\t\\n'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b9xt-tqOlce"
      },
      "source": [
        "Precisamos ainda de definir o comportamento do *tokenizer* caso encontre um carácter ou sequência de caracteres que não corresponda a nenhum *token* conhecido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYqjFDTEOlcp"
      },
      "outputs": [],
      "source": [
        "def t_error(t):\n",
        "    print(f\"Carácter ilegal {t.value[0]}\")\n",
        "    t.lexer.skip(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hMOGfxsOldI"
      },
      "source": [
        "Agora, já somos capazes de construir o nosso analisador léxico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN9vAe7UOldP"
      },
      "outputs": [],
      "source": [
        "lexer = lex.lex()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjACXglAOldP"
      },
      "source": [
        "Para o usar, precisamos de lhe dar algum valor de *input* e depois pedir-lhe para ir devolvendo os *tokens* que encontrar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5BEZj6XOldQ"
      },
      "outputs": [],
      "source": [
        "data = '''\n",
        "3 + 4 * 10\n",
        "  + -20 *2\n",
        "'''\n",
        "\n",
        "lexer.input(data)\n",
        "\n",
        "while tok := lexer.token():\n",
        "    print(tok)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Uvix55yOldz"
      },
      "source": [
        "Se quisermos manter informação sobre as linhas nas quais os *tokens* aparecem, podemos usar o atributo `lineno`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5CFURcLOleU"
      },
      "outputs": [],
      "source": [
        "t_ignore = ' \\t'\n",
        "\n",
        "def t_newline(t):\n",
        "    r'\\n+'\n",
        "    t.lexer.lineno += len(t.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPrQMVnjOldQ"
      },
      "source": [
        "É possível consultar a documentação do *lex.py* em https://ply.readthedocs.io/en/latest/ply.html#lex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyge6OSnOldR"
      },
      "source": [
        "## Exercícios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3hIQxn8OldR"
      },
      "source": [
        "### 1. Frases\n",
        "\n",
        "Define um analisador léxico capaz de ler uma frase e de identificar os seus componentes (palavras, vírgulas, sinais de pontuação)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UST_08V9OldS"
      },
      "source": [
        "### 2. Listas Mistas\n",
        "\n",
        "Define um analisador léxico capaz de receber listas com números, palavras ou valores booleanos como input (e.g.: `[ 1,5, palavra, False ,3.14,   0, fim]`) e identificar os seus *tokens*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s__Lct3Oldh"
      },
      "source": [
        "### 3. JSON\n",
        "\n",
        "Define um analisador léxico capaz de ler ficheiros em formato JSON e identificar os seus *tokens*.\n",
        "\n",
        "Exemplo de um documento JSON:\n",
        "\n",
        "---\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"John Doe\",\n",
        "  \"age\": 21,\n",
        "  \"gender\": \"male\",\n",
        "  \"height\": 1.68,\n",
        "  \"address\": {\n",
        "    \"street\": \"123 Main Street\",\n",
        "    \"city\": \"New York\",\n",
        "    \"country\": \"USA\",\n",
        "    \"zip\": \"10001\"\n",
        "  },\n",
        "  \"married\": false,\n",
        "  \"hobbies\": [\n",
        "    {\n",
        "      \"name\": \"reading\",\n",
        "      \"books\": [\n",
        "        {\n",
        "          \"title\": \"Heartstopper: Volume 1\",\n",
        "          \"author\": \"Alice Oseman\",\n",
        "          \"genres\": [\"Graphic Novels\", \"Romance\", \"Queer\"]\n",
        "        },\n",
        "        {\n",
        "          \"title\": \"1984\",\n",
        "          \"author\": \"George Orwell\",\n",
        "          \"genres\": [\"Science Fiction\", \"Dystopia\", \"Politics\"]\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"name\": \"gaming\",\n",
        "      \"games\": [\n",
        "        {\n",
        "          \"title\": \"Portal 2\",\n",
        "          \"platform\": [\"PC\", \"PlayStation 3\", \"Xbox 360\"]\n",
        "        },\n",
        "        {\n",
        "          \"title\": \"Synth Riders\",\n",
        "          \"platform\": [\"PSVR\", \"PSVR2\", \"PCVR\", \"Oculus Quest\"]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTLcLtKAOldi"
      },
      "source": [
        "## Condições de contexto\n",
        "\n",
        "Para certos analisadores léxico, pode ser útil ter diferentes estados. Por exemplo, se definirmos um analisador léxico para um ficheiro XML, pode ser útil verificar se o nome usado para fechar uma *tag* foi o mesmo que foi usado para a abrir.\n",
        "\n",
        "Exemplo de parte de um ficheiro XML:\n",
        "\n",
        "```xml\n",
        "<pessoa>\n",
        "    <nome>Maria</nome>\n",
        "    <idade>32</idade>\n",
        "</pessoa>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDH2v3NjOldw"
      },
      "outputs": [],
      "source": [
        "import ply.lex as lex\n",
        "\n",
        "states = (\n",
        "    ('taga', 'exclusive'),\n",
        "    ('tagf', 'exclusive'), # num estado exclusivo, apenas aplicamos os tokens e regras para esse estado\n",
        "                           # por outro lado, num estado inclusivo, as regras e tokens desse estado juntam-se às outras regras e tokens\n",
        "                           # o estado inicial chama-se 'INITIAL' e não é preciso defini-lo\n",
        ")\n",
        "\n",
        "tokens = (\n",
        "    'ABRIR_TAG',      # '<'\n",
        "    'ABRIR_TAG_F',    # '</'\n",
        "    'FECHAR_TAG',     # '>'\n",
        "    'NOME_TAG',       # Palavra case-insensitive\n",
        "    'VALOR'           # quaisquer Carateres entre Tags\n",
        ")\n",
        "\n",
        "t_ignore = ' \\t\\n' # estes tokens apenas são ignorados no estado 'INITIAL' e em estados inclusivos\n",
        "\n",
        "t_VALOR = r'[^<]+'\n",
        "\n",
        "def t_ABRIR_TAG_F(t):\n",
        "    r'</'\n",
        "    t.lexer.begin('tagf') # entramos no estado 'tagf'\n",
        "    return t\n",
        "\n",
        "def t_ABRIR_TAG(t):\n",
        "    r'<'\n",
        "    t.lexer.begin('taga') # entramos no estado 'taga'\n",
        "    return t\n",
        "\n",
        "def t_taga_tagf_FECHAR_TAG(t):\n",
        "    r'>'\n",
        "    t.lexer.begin('INITIAL') # voltamos ao estado inicial\n",
        "    return t\n",
        "\n",
        "def t_taga_NOME_TAG(t):\n",
        "    r'\\w+'\n",
        "    t.lexer.stack.append(t.value)\n",
        "    return t\n",
        "\n",
        "def t_tagf_NOME_TAG(t):\n",
        "    r'\\w+'\n",
        "    if len(t.lexer.stack) > 0:\n",
        "        if (nt := t.lexer.stack.pop(-1)) != t.value:\n",
        "            print(f\"Erro - esperado nome de tag '{nt}', mas foi lido '{t.value}'!\")\n",
        "    else:\n",
        "        print(\"Erro - nenhuma tag aberta!\")\n",
        "    return t\n",
        "\n",
        "def t_ANY_error(t): # regra válida para todos os estados\n",
        "    print(f\"Carácter ilegal: {t.value[0]}\")\n",
        "    t.lexer.skip(1)\n",
        "\n",
        "\n",
        "data = '''\n",
        "<pessoa>\n",
        "    <nome>Maria</nome>\n",
        "    <idade>32</idade>\n",
        "</pessoa>\n",
        "'''\n",
        "\n",
        "lexer = lex.lex()\n",
        "\n",
        "lexer.stack = list() # vamos usar esta lista como stack para verificar os nomes das tags\n",
        "\n",
        "lexer.input(data)\n",
        "\n",
        "while tok := lexer.token():\n",
        "    print(tok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJqjWeSfOleV"
      },
      "source": [
        "## Exercícios 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onMLXP8gOleY"
      },
      "source": [
        "### 1. BibTeX\n",
        "\n",
        "Define um analisador léxico capaz de ler um ficheiro no formato *BibTeX* e identificar os seus *tokens*.\n",
        "\n",
        "Exemplo de um ficheiro BibTeX:\n",
        "\n",
        "---\n",
        "\n",
        "```bibtex\n",
        "@incollection {HDYE78,\n",
        "author = \"Ricardo Martini and Pedro Rangel Henriques and Giovani Libreloto\",\n",
        "title = \"Storing Archival Emigration Documents to Create Virtual Exhibition Rooms\",\n",
        "booktitle = \"New Contributions in Information Systems and Technologies\",\n",
        "series=\"Advances in Intelligent Systems and Computing\",\n",
        "editor=\"Rocha, Alvaro and Correia, Ana and Costanzo, S. and Reis, Luis Paulo\",\n",
        "volume=\"353\",\n",
        "pages=\"403-409\",\n",
        "year = \"2015\",\n",
        "month =  \"April\"\n",
        "}\n",
        "\n",
        "\n",
        "@book {H787,\n",
        "author = {Vitor T. Martins and Pedro Rangel Henriques and Daniela da Cruz},\n",
        "title = {An AST-based tool, Spector, for Plagiarism Detection},\n",
        "booktitle = {Proceedings of SLATE’15},\n",
        "pages = {173--178},\n",
        "ISBN = {},\n",
        "year = {2015},\n",
        "month =   {},\n",
        "publisher = {Fundacion General UCM},\n",
        "annote = {Keywords: software, plagiarism, detection, comparison, test}}\n",
        "\n",
        "@book {H787,\n",
        "author = {Vitor T. Martins and Pedro Rangel Henriques and Daniela da Cruz},\n",
        "title = \"{A}n {AST}-based tool, {S}pector, for Plagiarism Detection\",\n",
        "booktitle = {Proceedings of SLATE’15},\n",
        "pages = {173--178},\n",
        "ISBN = {},\n",
        "year = {2015},\n",
        "month =   {},\n",
        "publisher = {Fundaci ́on General UCM},\n",
        "annote = {Keywords: software, plagiarism, detection, comparison, test}\n",
        "}\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxXn9uTSOlep"
      },
      "source": [
        "### 2. Somador on/off\n",
        "\n",
        "Usando um analisador léxico com condições de contexto, cria um programa em Python que tenha o seguinte comportamento:\n",
        "\n",
        "* Pretende-se um programa que some todas as sequências de dígitos que encontre num texto;\n",
        "* Prepara o programa para ler o texto do canal de entrada: stdin;\n",
        "* Sempre que encontrar a string “Off” em qualquer combinação de maiúsculas e minúsculas, esse comportamento é desligado;\n",
        "* Sempre que encontrar a string “On” em qualquer combinação de maiúsculas e minúsculas, esse comportamento é novamente ligado;\n",
        "* Sempre que encontrar o caráter “=”, o resultado da soma é colocado na saída."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Removedor de Comentários\n",
        "\n",
        "Desenvolve um analisador léxico capaz de ler um ficheiro de texto e ignorar todo o texto dentro de comentários inline (desde \"//\" até ao fim de linha) e todo o texto dentro de comentários multiline (desde \"/*\" até \"*/\").\n",
        "\n",
        "O *lexer* deve suportar convenientemente comentários dentro de comentários,   conforme exemplificado abaixo:\n",
        "\n",
        "---\n",
        "```c\n",
        "/* comment */ ola1\n",
        "\n",
        "/* comment****comment */ ola2 /*\n",
        "comment\n",
        "/* comentário dentro de comentário */\n",
        "****/ ola3\n",
        "\n",
        "/*********/\n",
        "\n",
        "ola4\n",
        " mais um pouco // remover comentário inline\n",
        "FIM\n",
        "```\n",
        "----\n"
      ],
      "metadata": {
        "id": "JQePKCJEg8Oj"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5994ec7557f9f4a3c4b5d4f7132f657ee5f793fd2b9e6534077474582be4b489"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}